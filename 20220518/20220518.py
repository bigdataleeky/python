# -*- coding: utf-8 -*-
"""20220518.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1cx5k1Q5emucfFogsuqxSaIiQxYzUFVSo
"""

from sklearn import datasets
iris = datasets.load_iris()
iris.keys()

print(iris['DESCR'])

# 데이터 분석.... 판다스...
import pandas as pd
df = pd.DataFrame(iris['data'],columns= iris['feature_names'])
df.head()

df.columns = ['sepal_length','sepal_width','petal_length','petal_width']
df.head()

df['target'] =  iris['target']
df.head()

# EDA info()
df.info()

df.describe()  # 기술통계량

# 결측값 -- 주의  수치상 결측치가 없지만... 실제 의미상 존재 할수 있다.
# a001 ,a002  abcf
df.isnull().sum()

# 중복데이터 확인
df.duplicated().sum()

df[df.duplicated()]

df[(df.sepal_length == 5.8)&(df.sepal_width==2.7)&(df.petal_length==5.1)]

# df.drop_duplicates()
df.drop(index=142,inplace=True)
df.iloc[140:150,:]

import matplotlib.pyplot as plt

temp = df.boxplot(grid=False)
plt.show()

plt.boxplot( df.sepal_width)
plt.show()

# des = df.sepal_width.describe()
# print(des['75%'] - des['25%'])
# IRQ = des['75%'] - des['25%']
# df2 = df[(des['25%'] - IRQ <df.sepal_width) & (df.sepal_width< des['75%']+IRQ)]
# df2.head()

df2 = df;
df2.boxplot()
df2.shape

# 상관관계 분석
df2.iloc[:,:-1].corr()

# 상관관계 시각화 sns
import seaborn as sns
sns.heatmap(data=df2.iloc[:,:-1].corr(),square=True, annot=True,cbar=True)
plt.show()

# 목표레이블의 클래스별 분포...  데이터 불균형 문제 발생 - 좀더 지켜보자..
df2['target'].value_counts()

# 시본 displot함수  히스토그램
sns.displot(x='sepal_width', kind='hist', data = df2)
plt.show()

# 시본 displot함수  밀도
sns.displot(x='sepal_width', kind='kde', data = df2)
plt.show()

# 품종별로 데이터 분포  
sns.displot(x='sepal_width', hue='target' ,kind='kde', data = df2)
plt.show()

# 피처간 관계
sns.pairplot(df2,hue='target', diag_kind='kde',size=2.5)
plt.show()

# petal with & petal length
# classfication.... 
from sklearn.model_selection import  train_test_split
df2.columns
x_data = df2.loc[:,['petal_width','petal_length']]  #petal_length
y_data  = df2.loc[:,'target']
x_data.shape, y_data.shape

x_train,x_test,y_train,y_test =  train_test_split(x_data,y_data,random_state=32,test_size=0.3)
x_train.shape,x_test.shape,y_train.shape,y_test.shape

# 알고리즘.... 분류.... knn
from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier(n_neighbors=3)
knn.fit(x_train,y_train)

# 하이퍼파라메터튜닝 - 필요하면..
knn.get_params()

y_predic = knn.predict(x_test)
y_predic

# 성능 평가
from sklearn.metrics import accuracy_score
knn_acc = accuracy_score(y_test,y_predic)
print("Accuracy : %.4f"% knn_acc)

# 분류알고리즘 SVM
# svm :  데이터 셋의 각 피처벡터들이 고유의 축을 갖는 벡터 공간
# 이때 각 군집까지의 거리(margin)를 최대한 멀리 유지하는 경계면을 찾는다 - 분류가 명확해 진다.
from sklearn.svm import SVC
svc = SVC(kernel='rbf')
svc.fit(x_train, y_train)

# 예측
y_svc_pred =  svc.predict(x_test)
print("예측값:",y_svc_pred[:5])
print("실제값:",y_test.values[:5])
#성능평가
svc_acc = accuracy_score(y_test, y_svc_pred)
print("Accuracy:%.4f"%svc_acc)

y_test.values[:5]

# 로지스틱 회귀  - 회귀라는 이름을 가지지만 실제로는 분류
# 시그모이드 함수의 출력값(0~1사이) 각 분류 클래스에 속하게 될 확율값으로 사용
# 1에 가까우면 해당 클래스로 분류하고 0에 가까우면 아니라고 분류
# overfit이 발생하면...--- 규제알고리즘을 적용할수있다. L1, L2
from sklearn.linear_model import LogisticRegression
lrc = LogisticRegression()
lrc.fit(x_train,y_train)

y_lrc_pred =  lrc.predict(x_test)
print("예측값:",y_lrc_pred[:5])
# 성능평가
lrc_acc = accuracy_score(y_test, y_lrc_pred)
print("Accuracy:%.4f"%lrc_acc)

# predict_proba : 각 클래스에 속할 확률 값을 예측
y_lrc_prob = lrc.predict_proba(x_test)
y_lrc_prob

# 의사결정 나무
# 트리 알고리즘을 사용
# 트리의 각 분기점(node)에는 데이터셋의 피처(설명 변수)를 하나씩 위치
# 트리의 최대 깊이는 max_depth 3으로 설정 , 트리의 깊이가  깊어지면 훈련데이터에 과적합이 일어난다

from sklearn.tree import DecisionTreeClassifier
dtc = DecisionTreeClassifier(max_depth=3, random_state=42)
dtc.fit(x_train,y_train)

# 예측
y_dtc_pred =  dtc.predict(x_test)
print("예측값:",y_dtc_pred[:5])
#성능평가
dtc_acc = accuracy_score(y_test,y_dtc_pred)
print("Accuracy:%.4f"%dtc_acc)

# 앙상블 모델
# 여러개의 모델이 예측한 값을 결합하여 최종 예측값을 결정하는 방법
# 보팅(3개의 모델을사용) : 같은데이터를 사용해서 학습 옵션중에 hard를 설정하면 3개모델이 예측한 값 중에서 다수결로
# 분류를정한다  

# 배깅
# 서로다른 데이터를 샘플링하여 학습

# 보팅
from sklearn.ensemble import VotingClassifier
# voting 다수결은 hard    평균 soft

hvc = VotingClassifier(estimators=[('KNN',knn),('SVM',svc),('DT',dtc)], voting='hard')

hvc.fit(x_train,y_train)
y_hvc_pred = hvc.predict(x_test)
print("예측값:",y_hvc_pred[:5])
#성능평가
hvc_acc = accuracy_score(y_test,y_hvc_pred)
print("Accuracy:%.4f"%dtc_acc)

# 배깅 알고리즘을 이용한 - RandomForest
from sklearn.ensemble import RandomForestClassifier
rfc = RandomForestClassifier(n_estimators=50,max_depth=3,random_state=20)
rfc.fit(x_train,y_train)
y_rfc_pred = rfc.predict(x_test)
print("예측값:",y_rfc_pred[:5])
#성능평가
rfc_acc = accuracy_score(y_test,y_hvc_pred)
print("Accuracy:%.4f"%rfc_acc)

# 앙상블 모델
# 부스팅 - XGBoost
# 여러개의 약한 학습기(가벼운 모델)를 순차적으로 학습한다.
# 잘못 예측한 데이터에 대한 예측 오차를 줄일수 있는 방향으로 모델을 계속 업데이트 
# 여러모델을 동시에 학습하지 않고 순서대로 학습하는점에서 배깅과 다르다...

# XGBoost : 속도가 빠르고 예측력이 상당히 좋은 편
from xgboost import XGBClassifier
xgbc = XGBClassifier(n_estimators=50,max_depth=3,random_state=20)
xgbc.fit(x_train,y_train)
y_xgbc_pred =  xgbc.predict(x_test)
print("예측값:",y_xgbc_pred[:5])
#성능평가
xgbc_acc = accuracy_score(y_test,y_xgbc_pred)
print("Accuracy:%.4f"%xgbc_acc)

# 교차검증
# 학습데이터 일부를 검증데이터로 사용하는 방법을 hold-out 교차검증
# 검증데이터는 모델 학습에 사용되지 않은 데이터.. 모델의 일반화 성능을 평가하는데 사용
#  결국은 테스트 데이터에 대한 예측력을 높여준다.
# 위에서나눈  x_train을 7:3으로 분활

x_tr,x_val,y_tr,y_val =  train_test_split(x_train,y_train,test_size=0.3,random_state=20)

x_tr.shape,x_val.shape,y_tr.shape,y_val.shape

# 랜덤포레스트에 적용
rfc = RandomForestClassifier(max_depth=3,random_state=20)
rfc.fit(x_tr, y_tr)
#예측
y_tr_pred = rfc.predict(x_tr)
y_val_pred = rfc.predict(x_val)
#검증
tr_acc = accuracy_score(y_tr,y_tr_pred)
val_acc = accuracy_score(y_val,y_val_pred)
tr_acc, val_acc

#예측
y_test_pred = rfc.predict(x_test)
test_acc = accuracy_score(y_test,y_test_pred)
test_acc

# 교차검증 k-fold
# k-fold는 hold-out을 여러번 반복하는 방법
# 랜덤포레스트를 k-fold 교차 검증으로 평가

from sklearn.model_selection import KFold

kfold = KFold()
count = 1
result = []
for tr_index, val_idx in kfold.split(x_train, y_train):  
  # 훈련용 데이터와 검증용데이터를 행 인덱스 기준으로 추출
  x_tr,x_val = x_train.iloc[tr_index,:], x_train.iloc[val_idx,:]
  y_tr,y_val = y_train.iloc[tr_index], y_train.iloc[val_idx]
  # #학습
  rfc=RandomForestClassifier(max_depth=5,random_state=20)
  rfc.fit(x_tr,y_tr)
  #검증
  y_val_predic = rfc.predict(x_val)
  val_acc = accuracy_score(y_val,y_val_predic)
  result.append(val_acc)
  print(f"{count} Fold Accuracy:{val_acc}")
  count += 1

# 위의 5개의 검증 정확도의 평균
import numpy as np
mean_score = np.mean(result)
mean_score

# Regression...
from sklearn import datasets
housing = datasets.load_boston()

housing.keys()

data = pd.DataFrame(housing['data'],columns= housing['feature_names'])
target = pd.DataFrame(housing['target'],columns= ['Target'])
df.shape, target.shape

df = pd.concat([data,target],axis=1)
df.head()

# 데이터 탐색
# 기술통계
# 결측값
# 이상치

# 상관관계
df.corr()
#히트맵
plt.figure(figsize=(10,10))
sns.set(font_scale=0.8)
sns.heatmap(df.corr(),annot=True)#,cbar=False)
plt.show()

# 목표변수 Target열과 상관계수가 높은 순서대로 열 이름과 상관계수를 출력
df.corr()

# 상관관계가 높은 순서대로 열 이름과 상관 계수를 출력
df.corr().loc[:'LSTAT','Target'].abs().sort_values(ascending=False)

# Target 변수와 함께 상관 계수가 높은 순서대로 4개 피처를 추출
plot_cols = df.corr().loc[:'LSTAT','Target'].abs().sort_values(ascending=False).index[:4]
plot_df= df.loc[:,plot_cols]
plot_df = pd.concat([df['Target'],plot_df],axis=1)
plot_df.head()

# seaborn 함수의 regplot 함수로 선형회귀 선을 산점도에 표시
plt.figure(figsize=(10,10))
for idx, col in enumerate(plot_df.columns[1:]):
  ax1 = plt.subplot(2,2,idx+1)
  sns.regplot(x=col, y=plot_df.columns[0],data=plot_df, ax= ax1)
plt.show()

# target에 대해서 좀더 확인이 필요.. 분포를 시각화
# target 데이터 분포
sns.displot(x='Target',kind='hist',data=df)
plt.show()

# 데이터 전처리..
# -- 피처 스케일링..